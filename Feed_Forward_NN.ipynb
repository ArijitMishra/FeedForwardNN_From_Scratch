{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Khapra Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPndgD7FdxT6S1Rl2f5OtwP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArijitMishra/FeedForwardNN_From_Scratch/blob/main/Feed_Forward_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZcLMgSLzdQm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iECoDNXz2op",
        "outputId": "40f0945e-922b-433b-aee7-24e2705cd380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "  #This is a feed forward model network\n",
        "  #THe input is in transpose form, that is the row vector is one data point\n",
        "  #Final layer activation is softmax\n",
        "  n_layers =0\n",
        "  n_neurons = []\n",
        "  weights=[]\n",
        "  bias=[]\n",
        "  input_dimen = 784\n",
        "  loss=''\n",
        "  activation = ''\n",
        "\n",
        "  def __init__(self,n_neurons=[784,12,12,10], loss='ce', activation='sigmoid'): #n_neurons has the input and the output class neurons as well\n",
        "    self.n_layers = len(n_neurons)\n",
        "    self.n_neurons = n_neurons\n",
        "\n",
        "    for i in range(self.n_layers-1):\n",
        "      w = np.random.randn(self.n_layers[i+1],self.n_neurons[i])\n",
        "      b = np.random.randn(self.n_layers[i+1])\n",
        "      self.weights.append(w)\n",
        "      self.bias.append(b)\n",
        "    self.weights = np.array(self.weights)\n",
        "    self.bias = np.array(self.bias)\n",
        "\n",
        "    self.loss = loss\n",
        "    self.activation = activation\n",
        "\n",
        "  def sigmoid(self,x, derivative=False):\n",
        "    if derivative==False:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    else:\n",
        "      sig_x = self.sigmoid(x)\n",
        "      return sig_x*(1-sig_x)\n",
        "  \n",
        "  def softmax(self,x,derivative=False): #x is the output from all data points and is in transpose form\n",
        "    if derivative==False:\n",
        "      out = np.zeros(x.shape)\n",
        "\n",
        "      for i in range(x.shape[1]):\n",
        "        exps = np.exp(x[:,i]-np.max(x[:,i]))\n",
        "        out[:,i] = exps/np.sum(exps)\n",
        "      return out \n",
        "  \n",
        "  def forward(self,x):\n",
        "    ais = [] #Pre activation\n",
        "    his = []#Post activation\n",
        "\n",
        "    ais.append[x]\n",
        "    his.append[x]\n",
        "\n",
        "    for i in range(1,self.n_layers-1): #Hidden layer calculation\n",
        "      curr_ai = self.weights[i]@ais[i-1] + self.bias[i]\n",
        "      ais.append(curr_ai)\n",
        "      curr_hi = self.sigmoid(curr_ai) #with sigmoid activation\n",
        "      his.append(curr_hi)\n",
        "    \n",
        "    output_ai = self.weights[-1]@ais[self.n_layers-2] + self.bias[-1]\n",
        "    ais.append(ouput_ai)\n",
        "    output = self.softmax(output_ai)\n",
        "    his.append(output)\n",
        "    ais = np.array(ais)\n",
        "    his = np.array(his)\n",
        "\n",
        "    return ais,his\n",
        "  \n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mArAhig70LK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}